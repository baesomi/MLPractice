# -*- coding: utf-8 -*-
"""WineQulity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SEFD-Rviuxz6xpAEA9w_2PDq_FvZt1SE
"""

#Importing required packages.
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
# %matplotlib inline

# Loading dataset
wine = pd.read_csv("../data/Wine/winequality-red.csv")

wine.head()

wine.info()

# fixed acidity는 퀄리티에 영향 x
fig = plt.figure(figsize = (10, 6))
sns.barplot( x = 'quality', y = 'fixed acidity', data = wine)

# volatile acidity는 영향을 끼친다 퀄리티가 높을수록 낮은 경향
flg = plt.figure(figsize=(10,6))
sns.barplot( x = 'quality', y = 'volatile acidity', data = wine)

# critic acid(구연산)는 퀄리티가 높을수록 높은경향
fig = plt.figure(figsize=(10,6))
sns.barplot( x = 'quality', y = 'citric acid', data = wine)

# 여기서부터는 관계 있는것만  sulphates(황산염) 높을수록 퀄리티 높음
sns.barplot ( x = 'quality', y = 'sulphates', data= wine)

# chlorides (염화물) 은 낮을 수록 퀄리티가 높음
sns.barplot ( x = 'quality', y = 'chlorides', data= wine)

# alcohol 높을수록 퀄리티 높아짐
sns.barplot ( x = 'quality', y = 'alcohol', data= wine)

# 유의미한 컬럼 : volatile acidity , critic acidity, sulphates, chlorides, alcohol
# 낮을수록 퀄리티가 높은 컬럼 : volatile acidity, chlorides
# 높을수록 퀄리티가 낮은 컬럼 : critic acidity, sulphates, alcohol
# 별로 도움안될거같은 컬럼 드랍하기
list = ['fixed acidity', 'residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH' ]
wine = wine.drop(columns=list)

# 데이터 전처리
# 퀄리티를 2가지로 단순하게 good이랑 bad로 나눔
bins = (2, 6.5, 8)
group_names = ['bad', 'good']
wine['quality'] = pd.cut(wine['quality'], bins=bins, labels = group_names)

label_quality = LabelEncoder()
# bad는 0 good은 1
wine['quality'] = label_quality.fit_transform(wine['quality'])
wine['quality'].value_counts()

sns.countplot(wine['quality'])

# train, test 셋 나누기
X = wine.drop('quality', axis=1)
y = wine['quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# 최적화된 결과를 위해 standard scaling 적용
sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

# 데이터 준비완료! 머신러닝 알고리즘 시작
# 랜덤 포레스트 알고리즘 사용
rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(X_train, y_train)
pred_rfc = rfc.predict(X_test)

print(classification_report(y_test, pred_rfc))

print(confusion_matrix(y_test, pred_rfc))

# SGD (경사하강법) 적용해보자!
sgd = SGDClassifier(penalty=None)
sgd.fit(X_train,y_train)
pred_sgd = sgd.predict(X_test)

print(classification_report(y_test, pred_sgd))

# SVM
svc = SVC()
svc.fit(X_train, y_train)
pred_svc = svc.predict(X_test)

print(classification_report(y_test, pred_svc))

# Finding best parameter of SVC model
param = {
    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],
    'kernel':['linear', 'rbf'],
    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]
}

grid_svc = GridSearchCV(svc, param_grid=param, scoring='accuracy', cv=10)

grid_svc.fit(X_train, y_train)

# 그리드 서치는 하이퍼파라미터 최적화 하기위한 방법으로 개 노가다
grid_svc.best_params_

svc2 = SVC(C=1.2, gamma=1.1, kernel='rbf')
svc2.fit(X_train, y_train)
pred_svc2 = svc2.predict(X_test)

print(classification_report(y_test, pred_svc2)) # 그래도 2%나 올라감

